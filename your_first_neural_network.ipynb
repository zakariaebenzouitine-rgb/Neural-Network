{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your First Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ Goals of this challenge:\n",
    "- Writing the architecture of a Neural Network\n",
    "- Inspect some of the most important hyperparameters of a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üåó In this challenge, we will be working with the **moons dataset**. \n",
    "* Your goal is to build a Neural Network which separates two classes. \n",
    "* Each data point $X$ has two coordinates $X = (x_1, x_2)$ and belongs to either the class 0 or the class 1\n",
    "* These bi-dimensional points can be represented on a 2D-scatterplot, using colors to represent to two available classes $\\color{blue}{blue}$ and $\\color{red}{red} $\n",
    "* Here is an example of such a moon dataset:\n",
    "\n",
    "<img src=\"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/DL/moons_dataset.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1)  Generating the Moons dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö Here, we will use the Scikit-Learn `make_moons` function [(see documentation here)](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) to create 2 moons that cannot be linearly separated. \n",
    "\n",
    "üëâ Each moon corresponds to a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "### Generating the moons dataset\n",
    "X, y = make_moons(n_samples=300, noise=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This function allows to plot the two moons\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_moons(X, y):\n",
    "    df = pd.DataFrame(dict(x1=X[:,0], x2=X[:,1], label=y))\n",
    "    colors = {0:'red', 1:'blue'}\n",
    "    fig, ax = plt.subplots()\n",
    "    grouped = df.groupby('label')\n",
    "\n",
    "    for key, group in grouped:\n",
    "        group.plot(ax=ax, kind='scatter', x='x1', y='x2', label=key, color=colors[key])\n",
    "\n",
    "\n",
    "    plt.title(f\"{len(X)} moons\")\n",
    "    plt.show()\n",
    "\n",
    "plot_moons(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì  Play with the number of samples and the noise to discover the effects on the `moons dataset` ‚ùì\n",
    "\n",
    "üé® Plot the moons for different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Create 250 samples of the data with ‚ùì\n",
    "* a noise equal to 0.20\n",
    "* a random state equal to 0 to get the same results every time you re-run the notebook\n",
    "* and split the initial dataset into a train and test set (size: 70/30%) \n",
    "\n",
    "*Remark* : Please call the variables `X_train`, `y_train`, `X_test` and `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Designing a basic Neural Network  for a binary classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß† You will now define your first neural network.\n",
    "The architecture of your model should have: \n",
    "- a first layer with:\n",
    "    - 5 neurons\n",
    "    - a _relu_ activation function \n",
    "    - the correct input dimension\n",
    "- an output layer suited to your binary classification task.\n",
    "\n",
    "\n",
    "‚ùì Complete the next function with the previous architecture ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "\n",
    "    #############################\n",
    "    #  1 - Model architecture   #\n",
    "    #############################\n",
    "\n",
    "    pass  # YOUR CODE HERE\n",
    "\n",
    "    #############################\n",
    "    #  2 - Optimization Method  #\n",
    "    #############################\n",
    "    model.compile(loss='binary_crossentropy', # We've already mentioned this loss function in Logistic Regression\n",
    "                  optimizer='adam', # Optimizer in Deep Learning = solver in Machine Learning | Adam = our best friend\n",
    "                  metrics=['accuracy']) # Let's focus on the accuracy, our dataset is balanced\n",
    "\n",
    "    return model\n",
    "\n",
    "model = initialize_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<details>\n",
    "    <summary><i>Intuitions about the optimization method</i></summary>\n",
    "\n",
    "We'll keep this message short as you have a full course üìÜ **Deep Learning - Optimizers, Loss, Fitting** coming soon.\n",
    "    \n",
    "So, briefly speaking:\n",
    "    \n",
    "- the ***.compile()*** tells your algorithm how to optimize the weights of your network when fitting the network on real data\n",
    "    - the ***binary_crossentropy*** is the \"log-loss\" that you have already seen in Machine Learning in the context of a Logistic Regression for classification\n",
    "    - To date, the ***adam*** optimizer is the gold standard in Deep Learning - an optimizer in Deep Learning is like a solver in Machine Learning: it provides an iterative method to minimize the loss function of an algorithm with respect to its parameters\n",
    "    \n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "‚ùì How many parameters does the model have?  ‚ùì\n",
    "\n",
    "<details>\n",
    "    <summary><i>Hint</i></summary>\n",
    "\n",
    "The `.summary()`method displays:\n",
    "   - the stack of layers of your Neural Network\n",
    "   - the output shape after each layer\n",
    "   - the number of parameters for each layer (and consequently the total number of parameters)\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Training your Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìYou are now ready to train your algorithm. \n",
    "\n",
    "Let's go! Initialize your model and fit it on the training set using `100 epochs`! \n",
    "\n",
    "Store the results of the training into a `history` variable. ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÄ `history` contains information about the training.\n",
    "\n",
    "‚ùì Inspect all its attributes using `history.__dict__`. You will notice that you have access to epoch-by-epoch information stored in `history.history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Plot the history of the train loss using the following function ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('Train loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What do you observe ‚ùì"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "source": [
    "> YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è <u>Evaluation on unseen data</u> ‚ùóÔ∏è\n",
    "\n",
    "üßëüèª‚Äçüè´ A Deep Learning Model is like any other Machine Learning model. Once you've trained it on the training set and are satisfied with how the Neural Network was able to learn, you need to ask: can it be generalized to unseen data such as the test set? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Predict & Evaluate performance on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Predict the classes of the elements in the training set using _.predict()_ and store these predicted classes into a variable called `y_pred` ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What are your _loss_ and your _accuracy_ on the test set ‚ùì \n",
    "\n",
    "üí° In other words, you are asked to evaluate your Deep Learning model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üß™ Test your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('first_model', accuracy=accuracy)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Check the accuracy of your predictions visually by using our `plot_decision_regions` available in `utils/plots.py` ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üò• It looks like we are **underfitting** a bit, right? \n",
    "\n",
    "üòÅ We can try :\n",
    "- to train the model a bit longer...\n",
    "- ...or maybe to create a Deeper Neural network in the next section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Training your Neural Network with more epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Re-train the model for 500 iterations ‚ùì\n",
    "- Don't forget to call the `initialize_model` function, otherwise, your initial parameters will be those you have already learned on the previous _.fit()_ ...!)\n",
    "- Plot the history to see how the loss changed over the different epochs/iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Did the test accuracy improve? ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Not really... explanations here</i></summary>\n",
    "\n",
    "* The accuracy did not increase by training the model with more epochs...\n",
    "* So when should have we stopped the training of this Deep Learning Model?\n",
    "    * Answer in the next chapter <b><i>Deep Learning - Optimizers, Loss and Fitting</i></b>\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) A Deeper Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Let's try a **deeper** architecture that includes the following layers:\n",
    "\n",
    "- a first layer with 20 neurons (activation: `\"relu\"`)\n",
    "- a second layer with 10 neurons (activation: `\"relu\"`)\n",
    "- a third layer with 5 neurons (activation: `\"relu\"`)\n",
    "- an output layer suitable for this problem ‚ùì\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_2():\n",
    "\n",
    "    #############################\n",
    "    #  1 - Model architecture   #\n",
    "    #############################\n",
    "\n",
    "    pass  # YOUR CODE HERE\n",
    "\n",
    "    #############################\n",
    "    #  2 - Optimization Method  #\n",
    "    #############################\n",
    "    model.compile(loss='binary_crossentropy', # We've already mentioned this loss function in Logistic Regression\n",
    "                  optimizer='adam', # Optimizer in Deep Learning = solver in Machine Learning | Adam = our best friend\n",
    "                  metrics=['accuracy']) # Let's focus on the accuracy, our dataset is balanced\n",
    "\n",
    "    return model\n",
    "\n",
    "model = initialize_model_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What is the number of parameters of your new model ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Run your model on the previous dataset for 500 epochs and plot the loss afterwards ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What is your accuracy on the test test ? Store it as `accuracy_deep` variable (`float`) ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ **Test your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('deeper_model',\n",
    "                         accuracy=accuracy_deep)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Do you think we are overfitting on the noise ‚ùì Once again, use the `plot_decision_regions` function to help your intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "üèÅ Congratulations!\n",
    "\n",
    "üíæ Don't forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... and move on to the next challenge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
